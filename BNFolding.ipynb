{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scortex.io/batch-norm-folding-an-easy-way-to-improve-your-network-speed/\n",
    "def get_folded_weights(conv_weights, batch_norm_weights, epsilon=1e-3):\n",
    "    #original\n",
    "    # gamma = batch_norm_weights[0].reshape((1,1,1,batch_norm_weights[0].shape[0]))\n",
    "    #my method\n",
    "    gamma = batch_norm_weights[0].reshape((batch_norm_weights[0].shape[0]))\n",
    "    beta = batch_norm_weights[1]\n",
    "    mean = batch_norm_weights[2]\n",
    "    # original\n",
    "    # variance = batch_norm_weights[3].reshape((1,1,1,batch_norm_weights[3].shape[0]))\n",
    "    #my method\n",
    "    variance = batch_norm_weights[3].reshape((batch_norm_weights[3].shape[0]))\n",
    "    new_weight = conv_weights[0] * gamma / np.sqrt(variance+epsilon)\n",
    "    \n",
    "    new_bias = beta + (conv_weights[1]- mean) * gamma / np.sqrt(variance + epsilon)\n",
    "\n",
    "    #my addendum to fix shape from (1, 1, 1, 32) to (32,)\n",
    "    return new_weight, new_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use this for Linear Sequential Models ONLY\n",
    "1. Loops through all layers in original model and duplicate all layers except BatchNorm and Dropout\n",
    "2. Loops through layers in new model and grabs sequential Convolutions and BatchNorm weights and biases and then computes linear transformation\n",
    "3. Replaces applicable convolution layer weights with newly computed folded weights and biases\n",
    "\n",
    "* Since dense layers can be represented as 1x1 convolutions, dense layers can be folded with batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FoldBN(model, new_model):\n",
    "    '''Folds Batchnormalization layers into nearest convolution\n",
    "    Limitation is that activation cannot sit between conv and BN layer\n",
    "    '''\n",
    "    for idx, layer in enumerate(model.layers):\n",
    "        #Layer must be conv2D and have none/linear activation\n",
    "        if layer.__class__.__name__ == 'Conv2D':\n",
    "            if layer.activation.__name__=='linear':\n",
    "                if model.layers[idx+1].__class__.__name__ == 'BatchNormalization':\n",
    "                    Conv = layer\n",
    "                    ConvWeights = Conv.get_weights()\n",
    "\n",
    "                    BatchNorm = model.layers[idx+1]\n",
    "                    NormWeights = BatchNorm.get_weights()\n",
    "\n",
    "                    ConvWeight, ConvBias = get_folded_weights(ConvWeights, NormWeights)\n",
    "\n",
    "                    for new_layer in new_model.layers:\n",
    "                        if layer.name == new_layer.name:\n",
    "                            print(f'Updated {layer.name} weights and biases')\n",
    "                            new_layer.set_weights([ConvWeight, ConvBias])\n",
    "                            break\n",
    "        \n",
    "            elif model.layers[idx-1].__class__.__name__ == 'BatchNormalization':\n",
    "                Conv = layer\n",
    "                ConvWeights = Conv.get_weights()\n",
    "\n",
    "                BatchNorm = model.layers[idx-1]\n",
    "                NormWeights = BatchNorm.get_weights()\n",
    "\n",
    "                ConvWeight, ConvBias = get_folded_weights(ConvWeights, NormWeights)\n",
    "\n",
    "                for new_layer in new_model.layers:\n",
    "                    if layer.name == new_layer.name:\n",
    "                        print(f'Updated {layer.name} weights and biases')\n",
    "                        new_layer.set_weights([ConvWeight, ConvBias])\n",
    "                        break\n",
    "    return new_model\n",
    "\n",
    "def DuplicateModelLinear(model, without=['BatchNormalization', 'Dropout']):\n",
    "    '''Duplicates model other than layers listed in without argument\n",
    "    '''\n",
    "    new_model = tf.keras.Sequential([])\n",
    "    for layer in model.layers:\n",
    "        if not layer.__class__.__name__ in without:\n",
    "            new_model.add(layer)\n",
    "    optimizer = model.optimizer\n",
    "    loss = model.loss\n",
    "\n",
    "    new_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    print('x')\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = load_model('')\n",
    "new_model = DuplicateModelLinear(MODEL)\n",
    "new_model = FoldBN(MODEL, new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized BatchNorm Folding Technique\n",
    "Can be used on Linear and NonLinear Models\n",
    "1. Redefine model without BatchNorm or Dropout Layers and compile\n",
    "* Make sure model names match original models\n",
    "2. Name layers in pairs of (Conv2D, BatchNorm) in a list\n",
    "3. Copy original weights of kept layers from original to new model\n",
    "4. Loop through name list and fold pair together and assign folded weights and biases into the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_original_weights(model, new_model):\n",
    "    '''Takes new model and copies all relevant original weights into it\n",
    "    '''\n",
    "    for new_layer in new_model.layers:\n",
    "        old_layer = model.get_layer(new_layer.name)\n",
    "        new_layer.set_weights(old_layer.get_weights())\n",
    "    return new_model\n",
    "\n",
    "def FoldBN(model, new_model, layers):\n",
    "    '''Fold BatchNorm layers into decided convolution\n",
    "\n",
    "    Args:\n",
    "        model: original keras model\n",
    "        new_model: new keras model with batch norm layers removed\n",
    "        layers: tuple of tuples (Conv, BatchNorm)of layers that want to be\n",
    "            matched by name\n",
    "    '''\n",
    "    for layer_names in layers:\n",
    "        Conv = model.get_layer(layer_names[0])\n",
    "        Norm = model.get_layer(layer_names[1])    \n",
    "        Norm_weights = Norm.get_weights()\n",
    "        Conv_weights = Conv.get_weights()\n",
    "\n",
    "        NewWeight, NewBias = get_folded_weights(Conv_weights, Norm_weights)\n",
    "        layer = new_model.get_layer(Conv.name)\n",
    "        layer.set_weights([NewWeight, NewBias])\n",
    "        print(f'Layer weights from {layer_names[0]} & {layer_names[1]} are folded')\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#Import Relevant Layers\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Activation, Conv2D, Dense, Flatten, InputLayer, concatenate\n",
    "model = load_model('functional_split_net')\n",
    "# INPUT_SHAPE = (28,28,1)\n",
    "# NEW_MODEL = tf.keras.Sequential(\n",
    "#     [\n",
    "#     InputLayer(input_shape=INPUT_SHAPE),\n",
    "#     Conv2D(32, kernel_size=(3, 3), name='C1', activation=None),\n",
    "#     Activation('relu'),\n",
    "#     Conv2D(32, kernel_size=(3, 3), name='C2', activation=None),\n",
    "#     Activation('relu'),\n",
    "#     Conv2D(32, kernel_size=(3, 3), name='C3', activation=None),\n",
    "#     Activation('relu'),\n",
    "#     Conv2D(32, kernel_size=(3, 3), name='C4', activation=None),\n",
    "#     Activation('relu'),\n",
    "#     Conv2D(32, kernel_size=(3, 3), name='C5', activation=None),\n",
    "#     Activation('relu'),\n",
    "#     Conv2D(32, kernel_size=(3, 3), name='C6', activation=None),\n",
    "#     Activation('relu'),\n",
    "#     Conv2D(32, kernel_size=(3, 3), name='C7', activation=None),\n",
    "#     Activation('relu'),\n",
    "#     Flatten(name='flatten'),\n",
    "#     Dense(10, activation=\"softmax\", name='classifier'),\n",
    "#     ]\n",
    "# )\n",
    "# LAYERS = (('C1','BN1'),('C2','BN2'),('C3','BN3'),('C4','BN4'),('C5','BN5'),('C6','BN6'),('C7','BN7'))\n",
    "input = Input(shape=(28,28,1))\n",
    "x = Conv2D(32, kernel_size=(3, 3), name='C1', activation=None)(input)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(32, kernel_size=(3, 3), name='C2', activation=None)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "y = Conv2D(64, kernel_size=(3, 3), name='Cy1', activation=None)(x)\n",
    "y = Activation('relu')(y)\n",
    "y = Conv2D(64, kernel_size=(3, 3), name='Cy2', activation=None)(y)\n",
    "y = Activation('relu')(y)\n",
    "\n",
    "z = Conv2D(32, kernel_size=(3, 3), name='Cz1', activation=None)(x)\n",
    "z = Activation('relu')(z)\n",
    "z = Conv2D(32, kernel_size=(3, 3), name='Cz2', activation=None)(z)\n",
    "z = Activation('relu')(z)\n",
    "\n",
    "x = concatenate([y, z])\n",
    "x = Conv2D(32, kernel_size=(3, 3), name='C3', activation=None)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Flatten(name='flatten')(x)\n",
    "out = Dense(10, activation='softmax', name='classifier')(x)\n",
    "\n",
    "NEW_MODEL = Model(inputs=[input], outputs=[out])\n",
    "LAYERS = (('C1','BN1'),('C2','BN2'),('C3','BN3'),('Cy1','BNy1'),('Cy2','BNy2'),('Cz1','BNz1'),('Cz2','BNz2'))\n",
    "######################################\n",
    "optimizer = model.optimizer\n",
    "loss = model.loss\n",
    "NEW_MODEL.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "new_model_with_weights  = copy_original_weights(model, NEW_MODEL)\n",
    "NEW_MODEL = FoldBN(model, NEW_MODEL, LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(NEW_MODEL, 'functional_split_folded_wBN')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "918c1cd22d83585c6833aaad8c6f56967a29c60188e43e4a9798e1a86babfe21"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('RyanNet': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
